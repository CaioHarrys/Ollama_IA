# Ollama_IA ğŸš€ğŸ§ 

RepositÃ³rio para integraÃ§Ã£o e uso local de modelos de linguagem (LLMs) atravÃ©s do **[Ollama](https://ollama.com/)**, uma plataforma que permite rodar modelos de IA diretamente no seu computador, sem depender da nuvem.

---

## ğŸ“œ DescriÃ§Ã£o

Este projeto demonstra como utilizar a API do Ollama para interagir com modelos de linguagem de forma local via Python.

Com este script, vocÃª pode enviar perguntas ou comandos para um modelo AI rodando na sua mÃ¡quina, obtendo respostas rapidamente, com total controle dos dados e sem necessidade de conexÃ£o externa.

---

## ğŸ› ï¸ Tecnologias e Ferramentas

- ğŸ **Python 3.10+**
- ğŸ”— **API REST do Ollama**
- ğŸ“¦ **Bibliotecas Python:**
  - `requests`

---

## ğŸš€ InstalaÃ§Ã£o e ExecuÃ§Ã£o

### ğŸ”§ PrÃ©-requisitos
- Ter o **[Ollama](https://ollama.com/) instalado** na sua mÃ¡quina (Windows, macOS ou Linux).
- Ter um modelo baixado (ex.: `llama3`, `mistral`, `phi`, `llama2`, etc.).

### ğŸ“¥ Clone o repositÃ³rio:

git clone https://github.com/CaioHarrys/Ollama_IA.git
cd Ollama_IA

## ğŸ“¦ Instale as dependÃªncias:
- `pip install -r requirements.txt`

## â–¶ï¸ Execute o script:
- `python main.py`
## âš™ï¸ Como usar
- No arquivo main.py, vocÃª pode definir:

ğŸ”¸ O modelo que deseja utilizar (ex.: "llama3", "mistral", "llama2").

ğŸ”¸ A mensagem ou comando que serÃ¡ enviado ao modelo.
## ğŸ¯ Exemplo:
`model = "llama3"`
`prompt = "Explique o que Ã© aprendizado de mÃ¡quina de forma simples."`
`response = chat_with_ollama(model, prompt)`
`print(response)`
## ğŸ’¡ Funcionalidades
- âœ… Envio de prompts para modelos locais.

- âœ… Recebimento de respostas diretamente da API do Ollama.

- âœ… CÃ³digo simples e direto, fÃ¡cil de expandir para outros projetos.
## ğŸ¤ Contribuindo
ContribuiÃ§Ãµes sÃ£o bem-vindas! Sinta-se livre para abrir uma issue ou enviar um pull request.
## ğŸ§‘â€ğŸ’» Autor
Feito com ğŸ’™ por Caio Harrys